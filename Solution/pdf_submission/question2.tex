\section{Mistake Bound Model of Learning}\label{sec:q2}

\begin{enumerate}
    \item~
    
    
	\begin{enumerate}
		\item
			Since $1 \leq l \leq 80$, the size of concept class $|C| = 80$	
		
		\item
			The prediciton made by hypothesis of length $l$ on inputs $x_1^t$ and $x_2^t$ can be represented by the equation $$ sgn(2l - |x_1^t| - |x_2^t|) $$ 
			We make a mistake when this prediction does not match $sgn(y^t)$ of the label $y^t$. Hence we can determine whether mistake has been made using the following inequality:					
			\begin{equation}
				\boxed{				
					y^t(2l - |x_1^t| - |x_2^t|) < 0	
				}	
			\end{equation}
	
		\item
		Let us assume $l = l_c$ for correct function and $l = l_h$ for current hypothesis function.\\
		Let us consider what happens during positive and negative examples:
		\begin{enumerate}
			\item Positive Example: \\
			When there is a mistake on positive example, we have label $y^t = +1$ and equation $2l - |x_1^t| - |x_2^t| < 0$. Therefore we can say that either $x_1^t, x_2^t$ or both were greater than $l_h$ and we correct our hypothesis by setting $l_h = max(x_1^t, x_2^t)$ because the hypothesis will always make mistakes on all values of $l_h < max(x_1^t, x_2^t)$
			
			\item Negative Example: \\
			When there is a mistake on positive example, we have label $y^t = -1$ and equation $2l - |x_1^t| - |x_2^t| \geq 0$. For negatiive example i.e $y^t = -1$ we must have either $x_1^t, x_2^t$ or both greater than value of $l_c$ for the correct function. $l_h$ is higher than all of these values, therefore we set $l_h = max(x_1^t, x_2^t) - 1$. This will ensure our hypothesis matches the label for current example.
		\end{enumerate}
			
		\item 
		Let the $l = l_h$ for current hypothesis. $\{ .. \}$ represents comments in the pseudocode.\\
		\begin{algorithm}
			\caption{Mistake driven algorithm to learn correct function $f \in C$}
			\begin{algorithmic}
				\STATE Start with $l_h = 40$ \COMMENT {We set $l_h$ to half of the range of $l$}
				\FOR{each example}				
					\IF{$y^t(2l - |x_1^t| - |x_2^t|) < 0$}  
						\IF{$y^t == +1$} 
							\STATE $l_h = max(x_1^t, x_2^t)$ \COMMENT {If mistake is made on positive example}
						\ELSE
							\STATE $l_h = max(x_1^t, x_2^t) - 1$ \COMMENT {If mistake is made on negative example}
						\ENDIF				
					\ENDIF 
				\ENDFOR
			\end{algorithmic}	
		\end{algorithm}
	
	On any given dataset, the algorithm will make maximum of $m$ mistakes where $$m = |40 - l_c|$$ where $l_c$ is value of length $l$ for correct function $f \in C$.
	
	
	
	
	
	
	\end{enumerate}	    
    
     	
    
    
    

    \item~[20 points] Recall from class that the Halving algorithm assumes that there is the true (hidden) function is in the concept class $\mathcal{C}$ with $N$ elements and tries to find it. In this setting, we know the number of mistakes made by the algorithm is $O(\log N)$.
        Another way to think about this setting is that we are trying to predict with expert advice. That is, we have a pool of $N$ so called experts, only one of whom is perfect. As the halving algorithm proceeds, it cuts down this pool by at least half each time a mistake is made.

        Suppose, instead of one perfect expert, we have M perfect experts in our pool. Show that the mistake bound of the same Halving algorithm in this case is $O(\log\frac{N}{M})$.
        (Hint: To show this, consider the stopping condition of the algorithm. At what point, will the algorithm stop making mistakes?)
\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:
