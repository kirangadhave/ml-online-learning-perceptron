\section{Mistake Bound Model of Learning}\label{sec:q2}

\begin{enumerate}
    \item~
    
    
	\begin{enumerate}
		\item
			Since $1 \leq l \leq 80$, the size of concept class $|C| = 80$	
		
		\item
			The prediciton made by hypothesis of length $l$ on inputs $x_1^t$ and $x_2^t$ can be represented by the equation $$ sgn(2l - |x_1^t| - |x_2^t|) $$ 
			We make a mistake when this prediction does not match $sgn(y^t)$ of the label $y^t$. Hence we can determine whether mistake has been made using the following inequality:					
			\begin{equation}
				\boxed{				
					y^t(2l - |x_1^t| - |x_2^t|) < 0	
				}	
			\end{equation}
	
		\item
		Let us assume $l = l_c$ for correct function and $l = l_h$ for current hypothesis function.\\
		Let us consider what happens during positive and negative examples:
		\begin{enumerate}
			\item Positive Example: \\
			When there is a mistake on positive example, we have label $y^t = +1$ and equation $2l - |x_1^t| - |x_2^t| < 0$. Therefore we can say that either $x_1^t, x_2^t$ or both were greater than $l_h$ and we correct our hypothesis by setting $l_h = max(x_1^t, x_2^t)$ because the hypothesis will always make mistakes on all values of $l_h < max(x_1^t, x_2^t)$
			
			\item Negative Example: \\
			When there is a mistake on positive example, we have label $y^t = -1$ and equation $2l - |x_1^t| - |x_2^t| \geq 0$. For negatiive example i.e $y^t = -1$ we must have either $x_1^t, x_2^t$ or both greater than value of $l_c$ for the correct function. $l_h$ is higher than all of these values, therefore we set $l_h = max(x_1^t, x_2^t) - 1$. This will ensure our hypothesis matches the label for current example.
		\end{enumerate}
			
		\item 
		Let the $l = l_h$ for current hypothesis. $\{ .. \}$ represents comments in the pseudocode.\\
		\begin{algorithm}
			\caption{Mistake driven algorithm to learn correct function $f \in C$}
			\begin{algorithmic}
				\STATE Start with $l_h = 40$ \COMMENT {We set $l_h$ to half of the range of $l$}
				\FOR{each example}				
					\IF{$y^t(2l - |x_1^t| - |x_2^t|) < 0$}  
						\IF{$y^t == +1$} 
							\STATE $l_h = max(x_1^t, x_2^t)$ \COMMENT {If mistake is made on positive example}
						\ELSE
							\STATE $l_h = max(x_1^t, x_2^t) - 1$ \COMMENT {If mistake is made on negative example}
						\ENDIF				
					\ENDIF 
				\ENDFOR
			\end{algorithmic}	
		\end{algorithm}
	
	On any given dataset, the algorithm will make maximum of $m$ mistakes where $$m = |40 - l_c|$$ where $l_c$ is value of length $l$ for correct function $f \in C$.
	
	\end{enumerate}	    
    
     	 
    
    

    \item
		In the halving algorithm we predict output which agrees with majority of functions in the current concept class $C$. If a mistake is made, we remove the functions which predicted the wrong output and repeat the process on next example. This leads to cutting down of experts by atleast half      each time a mistake is made. This process stops when we reach down to $1$ expert because he has predicted correct output for all the examples. Now if we have $M$ experts in the initial concept class $C$ we will stop the algorithm when we have cut down the pool of experts  to these final $M$ functions.\\
		Let $|C| = N$ be the initial size of concept class. When $0$ mistakes are made we have size of initial concept class $|C_0| = |C|$. When first mistake is made, $$|C_1| \leq \frac{1}{2}|C_0|$$ When 2 mistakes are made, $$|C_2| \leq \frac{1}{2^2}|C_0|$$. Let $n$ be the number of mistakes for which the size of concept class is cut down to $M$. Therefore, $$M = |C_n| \leq \frac{1}{2^n}|C_0|$$
		Now in worst case we commit $M =  \frac{1}{2^n}|C_0|$ mistakes. But $|C_0| = |C| = N$. Therefore,
		$$M = \frac{1}{2^n}N$$
		\begin{align*}
			\therefore 2^n = \frac{N}{M}
		\end{align*}
		\begin{align*}
			\therefore n = \log_2 \bigg(\frac{N}{M}\bigg)    
		\end{align*}
		\therefore \ Mistake bound of halving algorithm in case of M experts is $O(\log \frac{N}{M})$
\end{enumerate}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw"
%%% End:
